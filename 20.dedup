#!/bin/bash
#SBATCH --job-name=dedup
#SBATCH --partition="small"
#SBATCH --time=24:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem-per-cpu=1750
#SBATCH --output=logs/%x.out

set -euo pipefail
shopt -s failglob
source .env

L=$1
COLL=$2
TMPSFX=tmp_$SLURM_JOB_ID
DIR=$WORKSPACE/$COLL/$L
JSON_FILES=$DIR/scored.*.jsonl.zst
OUTPUT=$DIR/dedup.jsonl.zst

# Read the split output, compress it to temp, then move
compress-batch(){
    local file=$1
    # Remove 0s prefix
    local name=$(echo $file | sed -E "s/\.0+/./g")

    # compress stdin, write to a temp
    zstd -T64 -10 >$name.jsonl.zst.tmp

    # remove temp suffix
    mv $name.jsonl.zst.tmp $name.jsonl.zst
}
export -f compress-batch


if [ -f $DIR/queries.zst ]; then
    # Single query file
    QUERY_FILES=$DIR/queries.zst
    dedup $QUERY_FILES $JSON_FILES
else
    # Distributed index, multiple query files
    QUERY_FILES=$DIR/queries.[0-9]*.zst
    dedup <(zpaste $QUERY_FILES) $JSON_FILES
fi | split - \
    --numeric-suffixes=1 -a 8 -C 120G \
    --filter='compress-batch $FILE' \
    $DIR/dedup.

rm $QUERY_FILES
mv $OUTPUT.$TMPSFX $OUTPUT
