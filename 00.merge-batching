#!/bin/bash
#SBATCH --job-name=merge-batching
#SBATCH --partition="small"
#SBATCH --time=72:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=64
#SBATCH --mem-per-cpu=1750
##SBATCH --nodes=1
##SBATCH --ntasks=128
##SBATCH --mem=0
#SBATCH --output=logs/%x.out
# Run batching python scripts that reads url.gz and text.gz
# decodes base64 text and creates tsv zstd compressed batches
# with one paragraph per line, url and collection metadata
module load cray-python/3.9.12.1
module load parallel
source .env
set -euo pipefail

L=$1
COLL=$2
INPUT_DIR=${COLLECTIONS[$COLL]}
OUTPUT_DIR=$WORKSPACE/$COLL/$L
mkdir -p $OUTPUT_DIR

# Read the split output, compress it to temp, then move
compress-batch(){
    local file=$1
    # Remove 0s prefix, numbers need to be the same way as slurm array ids
    local name=$(echo $file | sed -E "s/\.0+/./g")

    # jump over to a ceertain batch if previous ran out of time
    #local number=${name#*batch.}
    #if [ $((number)) -lt 940 ]; then return 0; fi

    # Remove \b that keeps doc boundaries
    # then compress
    tr '\b' '\n'  | tr -s '\n' \
    | zstd -T64 -10 >$name.zst.tmp

    mv $name.zst.tmp $name.zst
}
export -f compress-batch

lang-dirs (){
    local lang=$2
    local dir=$1
    case "$lang" in
        he) echo $INPUT_DIR/iw/*/*;;
        nb) echo $INPUT_DIR/no/*/*;;
        zh) echo $INPUT_DIR/{zh,zh-Hant}/*/*;;
        hbs) echo $INPUT_DIR/{bs,cnr,hr,sr}/*/*;;
        *) echo $INPUT_DIR/$lang/*/*;;
    esac
}


# Run one merge.py for each warc2text batch file
# then re-split into batches of 60GB
# --group avoids documents being separated
# should be faster than --keep-order and
# we do not need to keep order of the batches
parallel -j60 --will-cite \
    --tmpdir $FLASH_TMP \
    --compress --compress-program zstd --group \
    python merge.py -c $COLL {} ::: `lang-dirs $INPUT_DIR $L` \
| split - \
    --numeric-suffixes=1 -a 8 -C 120G \
    --filter='compress-batch $FILE' \
    $OUTPUT_DIR/batch.
