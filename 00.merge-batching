#!/bin/bash
#SBATCH --job-name=merge-batching
#SBATCH --partition="standard"
#SBATCH --time=12:00:00
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=8
##SBATCH --mem-per-cpu=1750
#SBATCH --nodes=1
#SBATCH --ntasks=128
#SBATCH --mem=0
#SBATCH --output=logs/%x.out
# Run batching python scripts that reads url.gz and text.gz
# decodes base64 text and creates tsv zstd compressed batches
# with one paragraph per line, url and collection metadata
module load cray-python/3.9.12.1
module load parallel
source .env
set -euo pipefail

L=$1
COLLECTIONS=${@:2}

# Read the split output, compress it to temp, then move
compress-batch(){
    local file=$1
    # Remove 0s prefix, numbers need to be the same way as slurm array ids
    local name=$(echo $file | sed -E "s/\.0+/./g")
    zstd -T64 -10 >$name.zst.tmp
    mv $name.zst.tmp $name.zst
}
export -f compress-batch

# Run one merge.py for each warc2text batch file
# then re-split into batches of 60GB
parallel -j128 --will-cite python merge.py {} ::: $WARC2TEXT_DIR/*/*/$L \
| split - \
    --numeric-suffixes=1 -a 8 -C 60G \
    --filter='compress-batch $FILE' \
    $WORKSPACE/$L/batch.
