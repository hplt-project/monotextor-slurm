#!/bin/bash
#SBATCH --job-name=merge-batching
#SBATCH --partition="standard"
#SBATCH --time=12:00:00
##SBATCH --ntasks=1
##SBATCH --cpus-per-task=8
##SBATCH --mem-per-cpu=1750
#SBATCH --nodes=1
#SBATCH --ntasks=128
#SBATCH --mem=0
#SBATCH --output=logs/%x.out
# Run batching python scripts that reads url.gz and text.gz
# decodes base64 text and creates tsv zstd compressed batches
# with one paragraph per line, url and collection metadata
module load cray-python/3.9.12.1
module load parallel
source .env
set -euo pipefail

L=$1
COLLECTIONS=${@:2}

compress_batch(){
    local file=$1
    local name=$(echo $file | sed -E "s/\.0+/./g")
    zstd -T48 -10 >$name.zst.tmp
    mv $name.tmp $name
}

parallel -j 64 --will-cite python merge.py {} ::: $WARC2TEXT_DIR/*/*/$L \
| split - \
    --numeric-suffixes=1 -a 8 -C 10G \
    --filter='zstd -T48 -10 >$(echo $FILE | sed -E "s/\.0+/./g").zst' \
    $WORKSPACE/$L/batch.
