#!/bin/bash
#SBATCH --job-name=index
#SBATCH --partition="small"
#SBATCH --time=72:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=128
## 3750 mem to go for 512gb nodes
#SBATCH --mem-per-cpu=1750
#SBATCH --output=logs/%x.out

set -euo pipefail
shopt -s failglob
source .env

IFS=" " read -ra args <<< $HQ_ENTRY
LANG=${args[0]}
COLL=${args[1]}
INPUT_DIR=$WORKSPACE/batches
TMPSFX=tmp_$$
if [ "$COLL" == "global" ]; then
    JSON_FILES=`ls -1 $INPUT_DIR/*/$LANG/batch_*.jsonl.zst`
else
    JSON_FILES=`ls -1 $INPUT_DIR/$COLL/$LANG/batch_*.jsonl.zst`
fi

# Log which files are processed files
echo $JSON_FILES >&2

CLUSTER_DIR=$WORKSPACE/clusters
mkdir -p $CLUSTER_DIR
# If it's an array job, do distributed index
if [[ ${#args[@]} -eq 3 ]]; then
    band=${args[2]}
    PARAMS="-b $((band-1))"
    OUTPUT_FILE=$CLUSTER_DIR/clusters.$LANG.$COLL.$band.zst
else
    PARAMS=""
    OUTPUT_FILE=$CLUSTER_DIR/clusters.$LANG.$COLL.zst
fi

if test -s $OUTPUT_FILE; then
    echo Clusters file already exists, exiting... $OUTPUT_FILE >&2
    exit 0
fi

#case "$L" in
##    zh | ja | th)
##        PARAMS="$PARAMS --tokenizer char";;
#    *)
#        PARAMS="$PARAMS --tokenizer whitespace";;
#esac

# Build the index and save clusters array
singularity exec --bind $(pwd -P) --bind $INPUT_DIR --pwd $(pwd -P) monotextor.sif \
    mhindex --batch-size 20000 $PARAMS $JSON_FILES \
| zstd -10 -T64 \
>$OUTPUT_FILE.$TMPSFX \
|| {
    echo "Error in pipeline: ${PIPESTATUS[@]}" >&2
    exit 1
}

sync
stat $OUTPUT_FILE.$TMPSFX
mv $OUTPUT_FILE.$TMPSFX $OUTPUT_FILE
