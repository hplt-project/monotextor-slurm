#!/bin/bash
#SBATCH --job-name=dedup
#SBATCH --partition="small"
#SBATCH --time=72:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=128
#SBATCH --mem-per-cpu=1750
#SBATCH --output=logs/%x.out

set -euo pipefail
shopt -s failglob
source .env

IFS=" " read -ra args <<< $HQ_ENTRY
LANG=${args[0]}
COLL=${args[1]}
INPUT_DIR=$WORKSPACE/batches
TMPSFX=tmp_$$
if [ "$COLL" == "global" ]; then
    JSON_FILES=`ls -1 $INPUT_DIR/*/$LANG/batch_*.jsonl.zst`
else
    JSON_FILES=`ls -1 $INPUT_DIR/$COLL/$LANG/batch_*.jsonl.zst`
fi
OUTPUT_DIR=$WORKSPACE/dedup/$LANG/$COLL
mkdir -p $OUTPUT_DIR

# Log which files are processed
echo $JSON_FILES >&2

# Read the split output, compress it to temp, then move
compress-batch(){
    local file=$1
    # Remove 0s prefix
    local name=$(echo $file | sed -E "s/\_0+/_/g")

    # compress stdin, write to a temp
    zstd -T$SLURM_CPUS_ON_NODE -10 >$name.jsonl.zst.tmp

    # remove temp suffix
    sync
    stat $name.jsonl.zst.tmp
    mv $name.jsonl.zst.tmp $name.jsonl.zst
}
export -f compress-batch


# check if the task has already done
temps=$(find $OUTPUT_DIR -name "batch_*.jsonl.zst.tmp" | wc -l)
if [[ $temps -eq 0 ]] && ! [ -z "$(ls $OUTPUT_DIR)" ]; then
    echo "Task '$HQ_ENTRY' already done" >&2
    exit 0
fi

sing="singularity exec --bind $(pwd -P) --bind $WORKSPACE --pwd $(pwd -P) monotextor.sif"
CLUSTER_DIR=$WORKSPACE/clusters
CLUSTER_FILE=$CLUSTER_DIR/clusters.$LANG.$COLL.zst
if [ -f $CLUSTER_FILE ]; then
    # Single clusters file
    $sing dedup -c $CLUSTER_FILE $JSON_FILES
    #rm $CLUSTER_FILES
else
    # Distributed index, multiple clusters files
    CLUSTER_FILES=$CLUSTER_DIR/clusters.$LANG.$COLL.[0-9]*.zst
    $sing dedup -c <(cat $CLUSTER_FILES) $JSON_FILES
    #rm $CLUSTER_FILES
fi | split - \
    --numeric-suffixes=1 -a 8 -C 20G \
    --filter='compress-batch $FILE' \
    $OUTPUT_DIR/batch_
